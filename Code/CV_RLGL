#!/usr/bin/env python3

import pyrealsense2 as rs
import numpy as np
import rospy
import cv2
import argparse
from playsound import playsound

# from RLGL.srv import RobotSound
#from pupil_apriltags import Detector

class Comp_vis:
    def __init__(self):
        '''
        The initiation function of the computer vision. Initiates subscribers and variables
        INPUTS:
            none
        OUTPUTS:
            none
        '''

        self.__R_sound = rospy.ServiceProxy("/robot_sound",Empty)
        self.__pew_sound = rospy.ServiceProxy("/pew_sound",Empty)

        self.lower_bluemark = np.array([74,94,73])
        self.upper_bluemark = np.array([124,255,174])


        self.redlight_greenlight()



    # def on_low_H_thresh_trackbar(self,val):
    #     self.low_H
    #     self.high_H
    #     self.low_H = val
    #     self.low_H = min(self.high_H-1, self.low_H)
    #     cv2.setTrackbarPos(self.low_H_name, self.window_detection_name, self.low_H)
    # def on_high_H_thresh_trackbar(self,val):
    #     self.low_H
    #     self.high_H
    #     self.high_H = val
    #     self.high_H = max(self.high_H, self.low_H+1)
    #     cv2.setTrackbarPos(self.high_H_name, self.window_detection_name, self.high_H)
    # def on_low_S_thresh_trackbar(self,val):
    #     self.low_S
    #     self.high_S
    #     self.low_S = val
    #     self.low_S = min(self.high_S-1, self.low_S)
    #     cv2.setTrackbarPos(self.low_S_name, self.window_detection_name, self.low_S)
    # def on_high_S_thresh_trackbar(self,val):
    #     self.low_S
    #     self.high_S
    #     self.high_S = val
    #     self.high_S = max(self.high_S, self.low_S+1)
    #     cv2.setTrackbarPos(self.high_S_name, self.window_detection_name, self.high_S)
    # def on_low_V_thresh_trackbar(self,val):
    #     self.low_V
    #     self.high_V
    #     self.low_V = val
    #     self.low_V = min(self.high_V-1, self.low_V)
    #     cv2.setTrackbarPos(self.low_V_name, self.window_detection_name, self.low_V)
    # def on_high_V_thresh_trackbar(self,val):
    #     self.low_V
    #     self.high_V
    #     self.high_V = val
    #     self.high_V = max(self.high_V, self.low_V+1)
    #     cv2.setTrackbarPos(self.high_V_name, self.window_detection_name, self.high_V)

    
    def redlight_greenlight(self):
        '''
        CV function for detecting ball.
        INPUTS:
            none
        OUTPUTS:
            none
        '''



        self.max_value = 255
        self.max_value_H = 360//2
        self.low_H = 0
        self.low_S = 0
        self.low_V = 0
        self.high_H = self.max_value_H
        self.high_S = self.max_value
        self.high_V = self.max_value
        self.window_capture_name = 'Video Capture'
        self.window_detection_name = 'Object Detection'
        self.low_H_name = 'Low H'
        self.low_S_name = 'Low S'
        self.low_V_name = 'Low V'
        self.high_H_name = 'High H'
        self.high_S_name = 'High S'
        self.high_V_name = 'High V'
        
        # parser = argparse.ArgumentParser(description='Code for Thresholding Operations using inRange tutorial.')
        # parser.add_argument('--camera', help='Camera devide number.', default=0, type=int)
        # args = parser.parse_args()
        # cap = cv.VideoCapture(args.camera)

        # cv2.namedWindow(self.window_capture_name)
        # cv2.namedWindow(self.window_detection_name)
        # cv2.createTrackbar(self.low_H_name, self.window_detection_name , self.low_H, self.max_value_H, self.on_low_H_thresh_trackbar)
        # cv2.createTrackbar(self.high_H_name, self.window_detection_name , self.high_H, self.max_value_H, self.on_high_H_thresh_trackbar)
        # cv2.createTrackbar(self.low_S_name, self.window_detection_name , self.low_S, self.max_value, self.on_low_S_thresh_trackbar)
        # cv2.createTrackbar(self.high_S_name, self.window_detection_name , self.high_S, self.max_value, self.on_high_S_thresh_trackbar)
        # cv2.createTrackbar(self.low_V_name, self.window_detection_name , self.low_V, self.max_value, self.on_low_V_thresh_trackbar)
        # cv2.createTrackbar(self.high_V_name, self.window_detection_name , self.high_V, self.max_value, self.on_high_V_thresh_trackbar)
        
        
        # #Alignment inits
        pipeline = rs.pipeline()

        config = rs.config()

        pipeline_wrapper = rs.pipeline_wrapper(pipeline)
        pipeline_profile = config.resolve(pipeline_wrapper)
        device = pipeline_profile.get_device()
        device_product_line = str(device.get_info(rs.camera_info.product_line))
        

        found_rgb = False
        for s in device.sensors:
            if s.get_info(rs.camera_info.name) == 'RGB Camera':
                found_rgb = True
                break
        if not found_rgb:
            print("The demo requires Depth camera with Color sensor")
            exit(0)

        config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)

        if device_product_line == 'L500':
            config.enable_stream(rs.stream.color, 960, 540, rs.format.bgr8, 30)
        else:
            config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)


        # Start streaming
        profile = pipeline.start(config)
        intr_profile = profile.get_stream(rs.stream.color)
        intr = intr_profile.as_video_stream_profile().get_intrinsics()
        print(intr)
        # Getting the depth sensor's depth scale (see rs-align example for explanation)
        depth_sensor = profile.get_device().first_depth_sensor()
        depth_scale = depth_sensor.get_depth_scale()
        print("Depth Scale is: " , depth_scale)

        # We will be removing the background of objects more than
        #  clipping_distance_in_meters meters away
        clipping_distance_in_meters = 100 #1 meter
        clipping_distance = clipping_distance_in_meters / depth_scale


        # Create an align object
        # rs.align allows us to perform alignment of depth frames to others frames
        # The "align_to" is the stream type to which we plan to align depth frames.
        align_to = rs.stream.color
        align = rs.align(align_to)

        #cv2.imshow("name",0)
        #cv2.newWindow()
        #config.enable_record_to_file(cvRecording)

        # variable inits
        waiter = 0
        period = 500
        green_period = 150
        red_period = 100
        red_timer = 0
        red_state=0
        green_state = 0
        green_timer = 0
        timer = 0
        s_cx = 0 
        s_cy = 0 
        s_depth = 0 
        thresh = 10
        out = 0
        die_points = 0
        reset_flag = 0
        dead_timer = 0
        goal_dist = 1000


        # self.__R_sound()
        playsound('squid_doll.mp3')


        # Streaming loop
        try:
            while True:
                # print("checkpoint1")

                if (red_timer > red_period) or (dead_timer > red_period):

                    # playsound('robot_sound.mp3')
                    self.__R_sound()
                    timer = 0
                    reset_flag = 1
                    out = 0
                    green_state = 1
                    red_state = 0
                    red_timer = 0
                    dead_timer = 0
                

                if green_state ==1:
                    green_timer +=1
                    print(f"green timer {green_timer}")
                elif red_state==1:
                    red_timer +=1
                    print(f"Red timer {red_timer}")
                else:
                    dead_timer += 1
                    print(f"Dead_timer = {dead_timer}")


                # elif red_state==1:
                #     red_timer +=1
                #     print(f"Red timer {red_timer}")
                # else:
                #     print("no state")

                if green_timer > green_period:
                    green_state = 0
                    green_timer = 0
                    red_state = 1
                    reset_flag = 1
                

                timer +=1
                # print(f"Timer {timer}")
                # Get frameset of color and depth
                frames = pipeline.wait_for_frames()
                # print("Frames checkpoint")
                # frames.get_depth_frame() is a 640x360 depth image

                # Align the depth frame to color frame
                aligned_frames = align.process(frames)

                # print("aligned frames")
                # Get aligned frames
                aligned_depth_frame = aligned_frames.get_depth_frame() # aligned_depth_frame is a 640x480 depth image
                color_frame = aligned_frames.get_color_frame()

                # print("checkpoint pre if")

                # Validate that both frames are valid
                if not aligned_depth_frame or not color_frame:
                    print("continued")
                    continue

                # print("checkpoint pre get data")
                depth_image = np.asanyarray(aligned_depth_frame.get_data())
                color_image = np.asanyarray(color_frame.get_data())
                # print("checkpoint get data")
                # # Remove background - Set pixels further than clipping_distance to grey
                # grey_color = 153
                depth_image_3d = np.dstack((depth_image,depth_image,depth_image)) #depth image is 1 channel, color is 3 channels
                # color_image = np.where((depth_image_3d > clipping_distance) | (depth_image_3d <= 0), grey_color, color_image)
                # print("checkpoint depth image")
                hsv = cv2.cvtColor(color_image, cv2.COLOR_BGR2HSV)
                # print("checkpoint hsv")
                # frame_threshold = cv2.inRange(hsv, (low_H, low_S, low_V), (high_H, high_S, high_V))

                # print("checkpoint frame threshold")
                # define range of colors in HSV
                lower_blue = np.array([110,50,50])
                upper_blue = np.array([130,255,255])

                lower_purp = np.array([113,83,54])
                upper_purp = np.array([137,198,198])


                # lower_green = np.array([84,0,0])
                # upper_green = np.array([104,255,255])

                # lower_green = np.array([89,134,85])
                # upper_green = np.array([103,255,181])

                # lower_green = np.array([85,66,32])
                # upper_green = np.array([106,255,184])

                lower_green = np.array([87,165,19])
                upper_green = np.array([101,255,224])

                lower_white = np.array([72,0,109])
                upper_white = np.array([120,62,255])
                


                # print("checkpoint3")

                # Threshold the HSV image to get only purple colors
                mask = cv2.inRange(hsv, lower_green, upper_green)
                mask2 = cv2.inRange(hsv, lower_white, upper_white)
                # Bitwise-AND mask and original image
                res = cv2.bitwise_and(color_image,color_image, mask= mask+mask2)
                cv2.imshow('frame',color_image)
                cv2.imshow('green mask',mask)
                cv2.imshow('white mask',mask2)

                cv2.imshow('res',res)
                # cv2.imshow(window_detection_name, frame_threshold)
                

                contours, hierarchy = cv2.findContours(mask , cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
                #print(contours)
                # print("checkpoint")


                try:
                    
                    try:
                        c = max(contours,key=cv2.contourArea)
                        #print(f"this is c = {c}")
                    except:
                        print("the issue is with c")
                        c = c*0
                    try:
                        M = cv2.moments(c)
                    except:
                        print("the issue is with moments")
                    try:
                        cx = int(M['m10']/M['m00'])
                        cy = int(M['m01']/M['m00'])
                        center = (int(cx),int(cy))
                    except:
                        print("the issue is with cxcy")
                    cv2.circle(color_image, center, 10,(0,0,255),-1)
                    try:
                        rect = cv2.minAreaRect(c)
                        box = cv2.boxPoints(rect)
                        box = np.int0(box)
                        cv2.drawContours(color_image, [box],0,(0,0,255),2)
                        # ellipse = cv2.fitEllipse(c)
                        # cv2.ellipse(color_image,ellipse,(0,0,255),2)
                    except:
                        print("box or ellipse problem")
                    try:
                        depth = depth_image[cy][cx]
                        # print(f"pix coords {cx},{cy}")
                        print(f"Player distance is {depth}")
                    except:
                        print("depth issue")

                except:
                    print("sum ting wong")
               

                ##### Find movement algorithm##############################################
                #shape module
                # https://www.wikiwand.com/en/Image_moment
                # https://docs.opencv.org/3.0-beta/modules/shape/doc/shape_distances.html

                # ShapeContextDistanceExtractor
                # HausdorDistanceExtractor

                #they take 2 contrours and returns measure of dissimilarity 

                ##################################### sudo game logic
                if reset_flag == 1:
                    s_cx = cx
                    s_cy = cy 
                    s_depth = depth
                    reset_flag=0
                    print("reset")
                    
                # print(f"pen_coord: {pen_coord} vs coords: {cx}, {cy}, {depth}")
                # print(f"scx = {s_cx}, scy = {s_cy}, s_depth = {s_depth}")
                diff_cx = cx - s_cx
                diff_cy = cy - s_cy
                # diff_depth = depth - s_depth

                total_diff = abs(diff_cx) + abs(diff_cy)
                # total_diff = diff_cx + diff_cy + diff_depth
                # print(f"cx_diff {diff_cx}")
                # print(f"cy_diff {diff_cy}")
                # print(f"depth_diff {diff_depth}")
                # print(f"total_diff {total_diff}")


                if (total_diff > thresh) and red_state == 1:
                    out = 1
                    red_state = 0
                    die_points +=1
                    dead_timer = red_timer - 30
                else:
                    out = 0

                    # wait for sound play
                    # redlight() # function that senses movement within timeframe and outputs status
                    # wait(red_time)

                #     greenlight() # function to signify safe and moves on

        
                if out == 1:
                    cv2.drawContours(color_image, contours, -1, (0,0,255),3)
                    # print(f"Die you've died {die_points} times")
                else:
                    cv2.drawContours(color_image, contours, -1, (0,255,0),3)

                if out == 1:
                    # playsound('gunshot.mp3')
                    self.__pew_sound()


                    # timer = period+1
                
                try:
                    if (depth < goal_dist) and green_state == 1:
                        print("You won!")
                        break
                except:
                    print("no depth")
                
                if die_points == 3:
                    break



                print(f"You've died {die_points} times")
                # Render images:
                #   depth align to color on left
                #   depth on right

                
                # depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)

                # images = np.hstack((color_image, depth_colormap))

                cv2.namedWindow('Align Example', cv2.WINDOW_NORMAL)
                cv2.imshow('Align Example',color_image)
                # cv2.imshow('Align Example', images)
                key = cv2.waitKey(1)
                # Press esc or 'q' to close the image window
                if key & 0xFF == ord('q') or key == 27:
                    cv2.destroyAllWindows()
                    break
        except:
            print("first try fail")
        finally:
            pipeline.stop()
        
        print("You are dead")
        playsound('squid_game_intro.mp3')


if __name__=='__main__': # main run for node. inits node, runs class, and spins
  '''
  Your quintessential main function to run the class and init the node
  INPUTS:
      none
  OUTPUTS:
      Empty
  '''
  rospy.init_node('CV_node')
  Comp_vis()
  rospy.spin()